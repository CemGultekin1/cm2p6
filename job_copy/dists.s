#!/bin/bash
#SBATCH --time=1:30:00
#SBATCH --array=26,27,28,29,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,145,148,150,151,152,153,154,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,196,199,202,205,211,214,217,220,223,226,229,232,235,238,577,578,579,580,581,582,583,584,585,586,587,588,589,592,595,598,601,604,607,610,613,616,619,622,625,628,631,634,637,640,643,646,649,652,655,658,661,664,667,670,673,676,679,682,685,688,691,694,697,700,703,706,709,712,715,718,721,724,727,730,733,736,739,742,745,748,751,754,757,760,763,766,769,772,775,778,781,784,787,790,793,796,799,802,805,808,811,814,817,820,823,826,829,832,835,838,841,844,847,850,853,856,859,862,865,868,871,874,877,880,883,886,889,892,895,898,901,904,907,910,914,916,918,920,922,924,926,928,930,932,934,936,938,940,942,944,946,948,950,952,954,956,958,960,964,966,968,970,972,974,976,978,980,982,984,986,988,990,992,996,998,1000,1002,1004,1006,1008,1012,1014,1016,1018,1020,1022,1024,1026,1028,1030,1032,1036,1038,1040
#SBATCH --mem=80GB
#SBATCH --job-name=dists
#SBATCH --output=/scratch/cg3306/climate/outputs/slurm_logs/dists_%A_%a.out
#SBATCH --error=/scratch/cg3306/climate/outputs/slurm_logs/dists_%A_%a.err
#SBATCH --cpus-per-task=8
#SBATCH --nodes=1
#SBATCH --gres=gpu:1
#SBATCH --ntasks-per-node=1
echo "$(date)"
ARGS=$(sed -n "$SLURM_ARRAY_TASK_ID"p /scratch/cg3306/climate/cm2p6/jobs/dists.txt)
module purge
singularity exec --nv --overlay /scratch/cg3306/climate/cm2p6/overlay-15GB-500K.ext3:ro /scratch/work/public/singularity/cuda11.2.2-cudnn8-devel-ubuntu20.04.sif /bin/bash -c "\
	source /ext3/env.sh;\
	python3 run/analysis/distributional.py $ARGS --mode eval;\
	"
echo "$(date)"